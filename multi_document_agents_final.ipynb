{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034a7661",
      "metadata": {
        "id": "034a7661"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-core\n",
        "%pip install llama-index-agent-openai\n",
        "%pip install llama-index-readers-file\n",
        "%pip install llama-index-postprocessor-cohere-rerank\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install unstructured[html]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112",
      "metadata": {
        "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49893d69-c106-4169-92c3-6b5b751066e9",
      "metadata": {
        "id": "49893d69-c106-4169-92c3-6b5b751066e9"
      },
      "outputs": [],
      "source": [
        "domain = \"docs.llamaindex.ai\"\n",
        "docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
        "!wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c661cb62-1e18-410c-bc2e-e707b66596a3",
      "metadata": {
        "id": "c661cb62-1e18-410c-bc2e-e707b66596a3"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.file import UnstructuredReader\n",
        "\n",
        "reader = UnstructuredReader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e",
      "metadata": {
        "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
        "all_files = [f.resolve() for f in all_files_gen]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d837b4b-130c-493c-b62e-6662904c20ca",
      "metadata": {
        "id": "3d837b4b-130c-493c-b62e-6662904c20ca"
      },
      "outputs": [],
      "source": [
        "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
      "metadata": {
        "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
        "outputId": "d1ddb244-1739-459c-84d3-e39edc04e7ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1656"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_html_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34cf357",
      "metadata": {
        "id": "a34cf357",
        "outputId": "5ca9217b-824d-4e2c-e3db-8ed2cbf56dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "680\n"
          ]
        }
      ],
      "source": [
        "useful_files = [\n",
        "    x\n",
        "    for x in all_html_files\n",
        "    if \"understanding\" in str(x).split(\".\")[-2]\n",
        "    or \"examples\" in str(x).split(\".\")[-2]\n",
        "]\n",
        "print(len(useful_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5",
      "metadata": {
        "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "doc_limit = 100\n",
        "\n",
        "docs = []\n",
        "for idx, f in enumerate(useful_files):\n",
        "    if idx > doc_limit:\n",
        "        break\n",
        "    print(f\"Idx {idx}/{len(useful_files)}\")\n",
        "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
        "\n",
        "    loaded_doc = Document(\n",
        "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs]),\n",
        "        metadata={\"path\": str(f)},\n",
        "    )\n",
        "    print(loaded_doc.metadata[\"path\"])\n",
        "    docs.append(loaded_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65161c88",
      "metadata": {
        "id": "65161c88",
        "outputId": "469536b6-f893-4664-e6de-d05a4c60bbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e56afdc",
      "metadata": {
        "id": "4e56afdc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
      "metadata": {
        "id": "dd6e5e48-91b9-4701-a85d-d98c92323350"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = OpenAIEmbedding(\n",
        "    model=\"text-embedding-3-small\", embed_batch_size=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
      "metadata": {
        "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "async def build_agent_per_doc(nodes, file_base):\n",
        "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
        "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
        "    if not os.path.exists(vi_out_path):\n",
        "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
        "        vector_index = VectorStoreIndex(nodes)\n",
        "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
        "    else:\n",
        "        vector_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
        "        )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "\n",
        "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\", llm=llm\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(summary_out_path):\n",
        "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        summary = str(\n",
        "            await summary_query_engine.aquery(\n",
        "                \"Extract a concise 1-2 line summary of this document\"\n",
        "            )\n",
        "        )\n",
        "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
        "    else:\n",
        "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
        "\n",
        "    query_engine_tools = [\n",
        "        QueryEngineTool.from_defaults(\n",
        "            query_engine=vector_query_engine,\n",
        "            name=f\"vector_tool_{file_base}\",\n",
        "            description=f\"Useful for questions related to specific facts\",\n",
        "        ),\n",
        "        QueryEngineTool.from_defaults(\n",
        "            query_engine=summary_query_engine,\n",
        "            name=f\"summary_tool_{file_base}\",\n",
        "            description=f\"Useful for summarization questions\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    function_llm = OpenAI(model=\"gpt-4\")\n",
        "    agent = FunctionAgent(\n",
        "        tools=query_engine_tools,\n",
        "        llm=function_llm,\n",
        "        system_prompt=f\"\"\"\\\n",
        "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
        "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    return agent, summary\n",
        "\n",
        "\n",
        "async def build_agents(docs):\n",
        "    node_parser = SentenceSplitter()\n",
        "\n",
        "    agents_dict = {}\n",
        "    extra_info_dict = {}\n",
        "\n",
        "    # all_nodes = []\n",
        "\n",
        "    for idx, doc in enumerate(tqdm(docs)):\n",
        "        nodes = node_parser.get_nodes_from_documents([doc])\n",
        "        # all_nodes.extend(nodes)\n",
        "\n",
        "        file_path = Path(doc.metadata[\"path\"])\n",
        "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
        "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
        "\n",
        "        agents_dict[file_base] = agent\n",
        "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
        "\n",
        "    return agents_dict, extra_info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f",
      "metadata": {
        "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f"
      },
      "outputs": [],
      "source": [
        "agents_dict, extra_info_dict = await build_agents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
      "metadata": {
        "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "\n",
        "def get_agent_tool_callable(agent: FunctionAgent) -> Callable:\n",
        "    async def query_agent(query: str) -> str:\n",
        "        response = await agent.run(query)\n",
        "        return str(response)\n",
        "\n",
        "    return query_agent\n",
        "\n",
        "\n",
        "all_tools = []\n",
        "for file_base, agent in agents_dict.items():\n",
        "    summary = extra_info_dict[file_base][\"summary\"]\n",
        "    async_fn = get_agent_tool_callable(agent)\n",
        "    doc_tool = FunctionTool.from_defaults(\n",
        "        async_fn,\n",
        "        name=f\"tool_{file_base}\",\n",
        "        description=summary,\n",
        "    )\n",
        "    all_tools.append(doc_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346ed0e1-b96f-446b-a768-4f11a9a1a7f6",
      "metadata": {
        "id": "346ed0e1-b96f-446b-a768-4f11a9a1a7f6",
        "outputId": "cc36cdc3-7212-4c93-a341-debaeb854123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToolMetadata(description='The document provides a series of tutorials on building agentic LLM applications using LlamaIndex, covering key steps such as building RAG pipelines, agents, and workflows, along with techniques for data ingestion, indexing, querying, and application evaluation.', name='tool_understanding_index', fn_schema=<class 'llama_index.core.tools.utils.tool_understanding_index'>, return_direct=False)\n"
          ]
        }
      ],
      "source": [
        "print(all_tools[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f",
      "metadata": {
        "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import (\n",
        "    ObjectIndex,\n",
        "    ObjectRetriever,\n",
        ")\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.schema import QueryBundle\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4o\")\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")\n",
        "vector_node_retriever = obj_index.as_node_retriever(\n",
        "    similarity_top_k=10,\n",
        ")\n",
        "\n",
        "class CustomObjectRetriever(ObjectRetriever):\n",
        "    def __init__(\n",
        "        self,\n",
        "        retriever,\n",
        "        object_node_mapping,\n",
        "        node_postprocessors=None,\n",
        "        llm=None,\n",
        "    ):\n",
        "        self._retriever = retriever\n",
        "        self._object_node_mapping = object_node_mapping\n",
        "        self._llm = llm or OpenAI(\"gpt-4o\")\n",
        "        self._node_postprocessors = node_postprocessors or []\n",
        "\n",
        "    def retrieve(self, query_bundle):\n",
        "        if isinstance(query_bundle, str):\n",
        "            query_bundle = QueryBundle(query_str=query_bundle)\n",
        "\n",
        "        nodes = self._retriever.retrieve(query_bundle)\n",
        "        for processor in self._node_postprocessors:\n",
        "            nodes = processor.postprocess_nodes(\n",
        "                nodes, query_bundle=query_bundle\n",
        "            )\n",
        "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
        "\n",
        "        sub_agent = FunctionAgent(\n",
        "            name=\"compare_tool\",\n",
        "            description=f\"\"\"\\\n",
        "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
        "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
        "\"\"\",\n",
        "            tools=tools,\n",
        "            llm=self._llm,\n",
        "            system_prompt=\"\"\"You are an expert at comparing documents. Given a query, use the tools provided to compare the documents and return a summary of the results.\"\"\",\n",
        "        )\n",
        "\n",
        "        async def query_sub_agent(query: str) -> str:\n",
        "            response = await sub_agent.run(query)\n",
        "            return str(response)\n",
        "\n",
        "        sub_question_tool = FunctionTool.from_defaults(\n",
        "            query_sub_agent,\n",
        "            name=sub_agent.name,\n",
        "            description=sub_agent.description,\n",
        "        )\n",
        "        return tools + [sub_question_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2",
      "metadata": {
        "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2"
      },
      "outputs": [],
      "source": [
        "custom_obj_retriever = CustomObjectRetriever(\n",
        "    vector_node_retriever,\n",
        "    obj_index.object_node_mapping,\n",
        "    node_postprocessors=[CohereRerank(top_n=5, model=\"rerank-v3.5\")],\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
      "metadata": {
        "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
        "outputId": "b22c0684-08a1-4f64-c645-9c0c0acce9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "tmps = custom_obj_retriever.retrieve(\"hello\")\n",
        "\n",
        "print(len(tmps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
      "metadata": {
        "id": "fed38942-1e37-4c61-89fa-d2ef41151831"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import ReActAgent, FunctionAgent\n",
        "\n",
        "top_agent = FunctionAgent(\n",
        "    tool_retriever=custom_obj_retriever,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries about the documentation.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# top_agent = ReActAgent(\n",
        "#     tool_retriever=custom_obj_retriever,\n",
        "#     system_prompt=\"\"\" \\\n",
        "# You are an agent designed to answer queries about the documentation.\n",
        "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "# \"\"\",\n",
        "#     llm=llm,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368",
      "metadata": {
        "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368"
      },
      "outputs": [],
      "source": [
        "all_nodes = [\n",
        "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485",
      "metadata": {
        "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485"
      },
      "outputs": [],
      "source": [
        "base_index = VectorStoreIndex(all_nodes)\n",
        "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
      "metadata": {
        "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
        "outputId": "6a6749ad-e6f9-497e-f239-66444eb84812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tool call: tool_SimpleIndexDemoLlama2_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            "\n",
            "Tool call: tool_apps_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            "\n",
            "Tool call: tool_putting_it_all_together_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            "\n",
            "Tool call: tool_llamacloud_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            "\n",
            "Calling tool tool_SimpleIndexDemoLlama2_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            " Got response: With LlamaIndex, you can build a VectorStoreIndex. This involves setting up the necessary environment, loading documents into the index, and then querying the index for information. You need to instal\n",
            "\n",
            "Tool call: tool_using_llms_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            "\n",
            "Calling tool tool_llamacloud_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            " Got response: With LlamaIndex, you can build a system that connects to your data stores, automatically indexes them, and then queries the data. This is done by integrating LlamaCloud into your project. The system a\n",
            "\n",
            "Calling tool tool_apps_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            " Got response: With LlamaIndex, you can build a full-stack web application. You can integrate it into a backend server like Flask, package it into a Docker container, or use it directly in a framework such as Stream\n",
            "\n",
            "Calling tool tool_putting_it_all_together_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            " Got response: With LlamaIndex, you can build a variety of applications and tools. This includes:\n",
            "\n",
            "1. Chatbots: You can use LlamaIndex to create interactive chatbots.\n",
            "2. Agents: LlamaIndex can be used to build intel\n",
            "\n",
            "Calling tool tool_using_llms_index with args {'query': 'What can you build with LlamaIndex?'}\n",
            " Got response: With LlamaIndex, you can build a variety of applications by leveraging the various Language Model (LLM) integrations it supports. These include OpenAI, Anthropic, Mistral, DeepSeek, Hugging Face, and \n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.agent.workflow import (\n",
        "    AgentStream,\n",
        "    ToolCall,\n",
        "    ToolCallResult,\n",
        ")\n",
        "\n",
        "handler = top_agent.run(\n",
        "    \"What can you build with LlamaIndex?\",\n",
        ")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\n",
        "            f\"\\nCalling tool {ev.tool_name} with args {ev.tool_kwargs}\\n Got response: {str(ev.tool_output)[:200]}\"\n",
        "        )\n",
        "    elif isinstance(ev, ToolCall):\n",
        "        print(f\"\\nTool call: {ev.tool_name} with args {ev.tool_kwargs}\")\n",
        "    # Print the stream of the agent\n",
        "    # elif isinstance(ev, AgentStream):\n",
        "    #     print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "response = await handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
      "metadata": {
        "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
        "outputId": "e63f4062-c6c7-4c9e-fd4c-aae8888ed270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With LlamaIndex, you can build various applications and tools, including:\n",
            "\n",
            "1. **VectorStoreIndex**: Set up and query a VectorStoreIndex by loading documents and configuring the environment as per the documentation.\n",
            "   \n",
            "2. **Full-Stack Web Applications**: Integrate LlamaIndex into backend servers like Flask, Docker containers, or frameworks like Streamlit. Resources include guides for TypeScript+React, Delphic starter template, and Flask, Streamlit, and Docker integration examples.\n",
            "\n",
            "3. **Chatbots, Agents, and Unified Query Framework**: Create interactive chatbots, intelligent agents, and a unified query framework for handling different query types. LlamaIndex also supports property graphs and full-stack web applications.\n",
            "\n",
            "4. **Data Management with LlamaCloud**: Build systems that connect to data stores, automatically index data, and efficiently query it by integrating LlamaCloud into your project.\n",
            "\n",
            "5. **LLM Integrations**: Utilize various Language Model (LLM) integrations such as OpenAI, Anthropic, Mistral, DeepSeek, and Hugging Face. LlamaIndex provides a unified interface to access different LLMs, enabling you to select models based on their strengths and price points. You can use multi-modal LLMs for chat messages with text, images, and audio inputs, and even call tools and functions directly through API calls.\n",
            "\n",
            "These capabilities make LlamaIndex a versatile tool for building a wide range of applications and systems.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064ff6c",
      "metadata": {
        "id": "2064ff6c"
      },
      "outputs": [],
      "source": [
        "# access the tool calls\n",
        "# print(response.tool_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
      "metadata": {
        "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
        "outputId": "6a9a2e69-84a7-4b19-fcac-386a8471036c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With LlamaIndex, you can build a variety of applications and systems, including a full-stack web application, a chatbot, and a unified query framework over multiple indexes. You can also perform semantic searches, summarization queries, and queries over structured data like SQL or Pandas DataFrames. Additionally, LlamaIndex supports routing over heterogeneous data sources and compare/contrast queries. It provides tools and templates to help you integrate these capabilities into production-ready applications.\n"
          ]
        }
      ],
      "source": [
        "response = base_query_engine.query(\n",
        "    \"What can you build with LlamaIndex?\",\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
      "metadata": {
        "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
        "outputId": "a1748b81-0810-43b3-9ea0-f313c24038ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workflows and query engines serve different purposes in an application context:\n",
            "\n",
            "1. Workflows:\n",
            "   - Workflows are designed to manage the execution flow of an application by dividing it into sections triggered by events.\n",
            "   - They are event-driven and step-based, allowing for the management of application complexity by breaking it into smaller, more manageable pieces.\n",
            "   - Workflows focus on controlling the flow of application execution through steps and events.\n",
            "\n",
            "2. Query Engines:\n",
            "   - Query engines are tools used to process queries against a database or data source to retrieve specific information.\n",
            "   - They are primarily used for querying and retrieving data from databases.\n",
            "   - Query engines are focused on the retrieval, postprocessing, and response synthesis stages of querying.\n",
            "\n",
            "In summary, workflows are more about controlling the flow of application execution, while query engines are specifically designed for querying and retrieving data from databases.\n"
          ]
        }
      ],
      "source": [
        "response = await top_agent.run(\"Compare workflows to query engines\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
      "metadata": {
        "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
        "outputId": "898c2a7e-1977-42a8-fca2-f8e2e5d2cb19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The compact response synthesizer mode aims to produce concise and condensed responses, focusing on delivering the most relevant information in a brief format. On the other hand, the tree_summarize response synthesizer mode is designed to create structured and summarized responses, organizing information in a comprehensive manner. \n",
            "\n",
            "In summary, the compact mode provides brief and straightforward responses, while the tree_summarize mode offers more detailed and organized output for a comprehensive summary.\n"
          ]
        }
      ],
      "source": [
        "response = await top_agent.run(\n",
        "    \"Can you compare the compact and tree_summarize response synthesizer response modes at a very high-level?\"\n",
        ")\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}